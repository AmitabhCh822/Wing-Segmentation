{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae241ed",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e747580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528ca8e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512a62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing Functions\n",
    "# Preprocessing with nan handling\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Perform preprocessing on image:\n",
    "    1. Grayscale conversion\n",
    "    2. Normalization\n",
    "    3. CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    4. Gamma transformation\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array or PIL image)\n",
    "    Returns:\n",
    "        Preprocessed image\n",
    "    \"\"\"\n",
    "    # Convert to numpy if PIL image\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale if RGB (using the formula from paper)\n",
    "    if len(image.shape) == 3:\n",
    "        # Using the formula: I_gray = 0.299*R + 0.587*G + 0.114*B\n",
    "        gray = 0.299 * image[:,:,0] + 0.587 * image[:,:,1] + 0.114 * image[:,:,2]\n",
    "        image = gray.astype(np.uint8)\n",
    "    \n",
    "    # Normalize with nan handling\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    \n",
    "    # Handle zero or very small std to prevent division by zero\n",
    "    if std < 1e-5:\n",
    "        norm_img = np.zeros_like(image, dtype=float)\n",
    "    else:\n",
    "        norm_img = (image - mean) / std\n",
    "    \n",
    "    # Replace inf/nan values\n",
    "    norm_img = np.nan_to_num(norm_img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Convert to uint8 for CLAHE (safely)\n",
    "    norm_min = norm_img.min()\n",
    "    norm_max = norm_img.max()\n",
    "    \n",
    "    # Handle case where min == max\n",
    "    if np.abs(norm_max - norm_min) < 1e-5:\n",
    "        norm_scaled = np.zeros_like(norm_img, dtype=np.uint8)\n",
    "    else:\n",
    "        norm_scaled = ((norm_img - norm_min) * 255 / (norm_max - norm_min)).astype(np.uint8)\n",
    "    \n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_img = clahe.apply(norm_scaled)\n",
    "    \n",
    "    # Apply gamma transformation (optional, gamma=1.2 for example)\n",
    "    gamma = 1.2\n",
    "    gamma_corrected = np.power(enhanced_img / 255.0, gamma) * 255.0\n",
    "    gamma_corrected = gamma_corrected.astype(np.uint8)\n",
    "    \n",
    "    return gamma_corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21475b",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929d9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset with Preprocessing\n",
    "class EnhancedVeinDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, mask_dir, transform=None, slice_size=48):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to CSV with BatID and ImageID columns\n",
    "            image_dir (str): Path to image folder\n",
    "            mask_dir (str): Path to mask folder\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            slice_size (int): Size of image patches to slice\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.slice_size = slice_size\n",
    "        \n",
    "        # Generate sliced patches\n",
    "        self.patches = self._generate_patches()\n",
    "        \n",
    "    def _generate_patches(self):\n",
    "        \"\"\"Generate patches from original images for data augmentation\"\"\"\n",
    "        patches = []\n",
    "        for idx in range(len(self.data)):\n",
    "            # Get image ID\n",
    "            image_id = self.data.iloc[idx]['ImageID']\n",
    "            \n",
    "            # Load image and mask\n",
    "            img_path = os.path.join(self.image_dir, f\"{image_id}.png\")\n",
    "            mask_path = os.path.join(self.mask_dir, f\"{image_id}.jpg\")\n",
    "            \n",
    "            # Open and preprocess image\n",
    "            image = Image.open(img_path).convert('L')\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "            \n",
    "            # Get image dimensions\n",
    "            width, height = image.size\n",
    "            \n",
    "            # Generate random patches (as in the paper Fig. 3)\n",
    "            # Number of patches per image - more patches for training\n",
    "            num_patches = 20  # Adjust as needed\n",
    "            \n",
    "            for _ in range(num_patches):\n",
    "                # Random center point within image\n",
    "                x = np.random.randint(self.slice_size//2, width - self.slice_size//2)\n",
    "                y = np.random.randint(self.slice_size//2, height - self.slice_size//2)\n",
    "                \n",
    "                # Crop patches\n",
    "                img_patch = image.crop((x - self.slice_size//2, y - self.slice_size//2,\n",
    "                                      x + self.slice_size//2, y + self.slice_size//2))\n",
    "                mask_patch = mask.crop((x - self.slice_size//2, y - self.slice_size//2,\n",
    "                                       x + self.slice_size//2, y + self.slice_size//2))\n",
    "                \n",
    "                patches.append((image_id, img_patch, mask_patch))\n",
    "        \n",
    "        return patches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, image_patch, mask_patch = self.patches[idx]\n",
    "        \n",
    "        # Preprocess image\n",
    "        preprocessed_img = preprocess_image(image_patch)\n",
    "        preprocessed_img = Image.fromarray(preprocessed_img)\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        image = TF.to_tensor(preprocessed_img)\n",
    "        mask = TF.to_tensor(mask_patch)\n",
    "        \n",
    "        # Apply additional transforms if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bac65",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860b8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Convolution Block with adaptive normalization based on spatial dimensions\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, min_spatial_dim=2):\n",
    "        \"\"\"\n",
    "        Double convolution block with adaptive normalization.\n",
    "        Uses BatchNorm for larger feature maps and GroupNorm for smaller ones.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            min_spatial_dim: Minimum spatial dimension to use BatchNorm (otherwise use GroupNorm)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # We'll choose normalization based on the output spatial dimensions\n",
    "        # (determined at forward time)\n",
    "        self.norm1_bn = nn.BatchNorm2d(out_channels)\n",
    "        # For GroupNorm, use 8 groups or out_channels if smaller\n",
    "        num_groups = min(8, out_channels)\n",
    "        self.norm1_gn = nn.GroupNorm(num_groups, out_channels)\n",
    "        \n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2_bn = nn.BatchNorm2d(out_channels)\n",
    "        self.norm2_gn = nn.GroupNorm(num_groups, out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.min_spatial_dim = min_spatial_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # Choose normalization based on spatial dimensions\n",
    "        if x.shape[2] >= self.min_spatial_dim and x.shape[3] >= self.min_spatial_dim:\n",
    "            # Use BatchNorm for larger feature maps\n",
    "            x = self.norm1_bn(x)\n",
    "        else:\n",
    "            # Use GroupNorm for smaller feature maps\n",
    "            x = self.norm1_gn(x)\n",
    "            \n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # Second convolution\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Choose normalization again\n",
    "        if x.shape[2] >= self.min_spatial_dim and x.shape[3] >= self.min_spatial_dim:\n",
    "            x = self.norm2_bn(x)\n",
    "        else:\n",
    "            x = self.norm2_gn(x)\n",
    "            \n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Weighted Feature Fusion for Bi-FPN\n",
    "class WeightedFeatureFusion(nn.Module):\n",
    "    def __init__(self, num_inputs, epsilon=0.0001):\n",
    "        super(WeightedFeatureFusion, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(num_inputs, dtype=torch.float32), requires_grad=True)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Apply ReLU to weights\n",
    "        weights = F.relu(self.weights)\n",
    "        \n",
    "        # Normalize weights\n",
    "        norm_weights = weights / (weights.sum() + self.epsilon)\n",
    "        \n",
    "        # Apply weights to inputs and sum\n",
    "        # Ensure all inputs have same shape before weighted fusion\n",
    "        # In this case, they should, but just to be safe:\n",
    "        if not all(inp.shape == inputs[0].shape for inp in inputs):\n",
    "            raise ValueError(f\"Input shapes don't match: {[inp.shape for inp in inputs]}\")\n",
    "            \n",
    "        # Weighted sum\n",
    "        out = sum(input_tensor * w for input_tensor, w in zip(inputs, norm_weights))\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Simplified BiFPN implementation that directly works with U-Net skip connections\n",
    "class BiFPNBlock(nn.Module):\n",
    "    def __init__(self, feature_dims):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dims: List of channel dimensions at each level (e.g. [32, 64, 128, 256, 512])\n",
    "                         Listed from highest resolution to lowest resolution\n",
    "        \"\"\"\n",
    "        super(BiFPNBlock, self).__init__()\n",
    "        self.feature_dims = feature_dims\n",
    "        \n",
    "        # For top-down path: project from higher level (fewer channels) to lower level (more channels)\n",
    "        # For U-Net skip connections in the order [fine_level, ..., coarse_level]\n",
    "        # We need to go from coarse_level -> fine_level in top-down path\n",
    "        \n",
    "        # Create a projection for each pair of adjacent levels\n",
    "        self.td_projections = nn.ModuleList()\n",
    "        for i in range(len(feature_dims) - 1):\n",
    "            # Project from level i+1 to level i (more channels to fewer channels)\n",
    "            self.td_projections.append(\n",
    "                nn.Conv2d(feature_dims[i+1], feature_dims[i], kernel_size=1)\n",
    "            )\n",
    "        \n",
    "        # For bottom-up path: project from lower level (more channels) to higher level (fewer channels)\n",
    "        self.bu_projections = nn.ModuleList()\n",
    "        for i in range(len(feature_dims) - 1):\n",
    "            # Project from level i to level i+1 (fewer channels to more channels)\n",
    "            self.bu_projections.append(\n",
    "                nn.Conv2d(feature_dims[i], feature_dims[i+1], kernel_size=1)\n",
    "            )\n",
    "        \n",
    "        # Convolutions after fusion for both paths\n",
    "        self.td_convs = nn.ModuleList()\n",
    "        self.bu_convs = nn.ModuleList()\n",
    "        \n",
    "        for dim in feature_dims:\n",
    "            # Process each feature level after fusion\n",
    "            self.td_convs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.bu_convs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Weighted fusion for each level\n",
    "        self.td_fusions = nn.ModuleList([WeightedFeatureFusion(2) for _ in range(len(feature_dims))])\n",
    "        self.bu_fusions = nn.ModuleList([WeightedFeatureFusion(2) for _ in range(len(feature_dims))])\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: list of features from encoder path, ordered from highest resolution \n",
    "                     to lowest resolution [fine_level, ..., coarse_level]\n",
    "        Returns:\n",
    "            Enhanced features with same ordering and dimensions\n",
    "        \"\"\"\n",
    "        num_levels = len(features)\n",
    "        \n",
    "        # Store intermediate features\n",
    "        td_features = [None] * num_levels  # Top-down path\n",
    "        bu_features = [None] * num_levels  # Bottom-up path\n",
    "        \n",
    "        # Top-down path (coarse -> fine)\n",
    "        # Start from the coarsest level\n",
    "        td_features[num_levels - 1] = features[num_levels - 1]\n",
    "        \n",
    "        # Process remaining levels from coarse to fine\n",
    "        for i in range(num_levels - 2, -1, -1):\n",
    "            # Get higher level feature and project to current dimension\n",
    "            higher_feature = td_features[i + 1]\n",
    "            projected_higher = self.td_projections[i](higher_feature)\n",
    "            \n",
    "            # Resize to current feature map size\n",
    "            resized_higher = F.interpolate(\n",
    "                projected_higher, \n",
    "                size=features[i].shape[2:],\n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "            # Fuse with current level feature\n",
    "            fused = self.td_fusions[i]([features[i], resized_higher])\n",
    "            td_features[i] = self.td_convs[i](fused)\n",
    "        \n",
    "        # Bottom-up path (fine -> coarse)\n",
    "        # Start from the finest level\n",
    "        bu_features[0] = td_features[0]\n",
    "        \n",
    "        # Process remaining levels from fine to coarse\n",
    "        for i in range(1, num_levels):\n",
    "            # Get lower level feature and project to current dimension\n",
    "            lower_feature = bu_features[i - 1]\n",
    "            projected_lower = self.bu_projections[i - 1](lower_feature)\n",
    "            \n",
    "            # Downsample to current feature map size\n",
    "            if projected_lower.shape[2:] != features[i].shape[2:]:\n",
    "                resized_lower = F.adaptive_max_pool2d(\n",
    "                    projected_lower, \n",
    "                    output_size=features[i].shape[2:]\n",
    "                )\n",
    "            else:\n",
    "                resized_lower = projected_lower\n",
    "            \n",
    "            # Fuse with current level feature\n",
    "            fused = self.bu_fusions[i]([td_features[i], resized_lower])\n",
    "            bu_features[i] = self.bu_convs[i](fused)\n",
    "        \n",
    "        return bu_features\n",
    "\n",
    "# Improved U-Net with Bi-FPN and adaptive normalization\n",
    "class UNetBiFPN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[32, 64, 128, 256, 512], max_levels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            features: Feature dimensions at each level\n",
    "            max_levels: Maximum number of downsampling levels to use (to prevent too small feature maps)\n",
    "        \"\"\"\n",
    "        super(UNetBiFPN, self).__init__()\n",
    "        \n",
    "        # If max_levels is provided, limit the feature levels\n",
    "        if max_levels is not None and max_levels < len(features):\n",
    "            features = features[:max_levels]\n",
    "            \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.features = features\n",
    "        \n",
    "        # Downsampling/Encoder path\n",
    "        in_feat = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_feat, feature))\n",
    "            in_feat = feature\n",
    "        \n",
    "        # Bottleneck - use GroupNorm for the bottleneck to avoid BatchNorm issues\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        \n",
    "        # Feature fusion with Bi-FPN\n",
    "        self.bifpn = BiFPNBlock(feature_dims=features)\n",
    "        \n",
    "        # Upsampling/Decoder path\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature * 2, feature, kernel_size=2, stride=2\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        \n",
    "        # Sigmoid activation for binary segmentation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Save original input dimensions to check how much we can downsample\n",
    "        orig_h, orig_w = x.shape[2], x.shape[3]\n",
    "        min_dim = min(orig_h, orig_w)\n",
    "        \n",
    "        # Calculate max possible downsampling levels (prevent feature maps < 2x2)\n",
    "        max_downsample = int(np.log2(min_dim)) - 1  # Leave at least 2x2\n",
    "        actual_levels = min(len(self.downs), max_downsample)\n",
    "        \n",
    "        # Store skip connections\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Encoder path - only go as deep as our input size allows\n",
    "        for i in range(actual_levels):\n",
    "            x = self.downs[i](x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Make sure skip_connections list has the right number of levels\n",
    "        skip_connections = skip_connections[:actual_levels]\n",
    "        \n",
    "        # Apply Bi-FPN to enhance skip connections, maintaining their original order\n",
    "        enhanced_skips = self.bifpn(skip_connections)\n",
    "        \n",
    "        # Reversed for decoder path (coarse to fine)\n",
    "        enhanced_skips = enhanced_skips[::-1]\n",
    "        \n",
    "        # Decoder path with enhanced skip connections\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)  # Upsample\n",
    "            \n",
    "            # Get corresponding skip connection\n",
    "            skip_idx = idx // 2\n",
    "            if skip_idx < len(enhanced_skips):\n",
    "                skip = enhanced_skips[skip_idx]\n",
    "                \n",
    "                # Handle case where sizes don't match exactly\n",
    "                if x.shape[2:] != skip.shape[2:]:\n",
    "                    x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Concatenate with enhanced skip connection\n",
    "                concat_skip = torch.cat((skip, x), dim=1)\n",
    "                \n",
    "                # Double convolution\n",
    "                x = self.ups[idx+1](concat_skip)\n",
    "            else:\n",
    "                # If we ran out of skip connections, just use upsampled x\n",
    "                x = self.ups[idx+1](torch.cat((x, x), dim=1))\n",
    "        \n",
    "        # Final 1x1 convolution and sigmoid\n",
    "        return self.sigmoid(self.final_conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f330a3",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96c301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions from the paper\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # Flatten predictions and targets\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate Dice coefficient\n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.2, bce_weight=0.8):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        dice_loss = self.dice(predictions, targets)\n",
    "        bce_loss = self.bce(predictions, targets)\n",
    "        \n",
    "        return self.dice_weight * dice_loss + self.bce_weight * bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549e805",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1fe572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create train/validation/test splits while keeping bats together\n",
    "def create_train_val_test_splits_enhanced(csv_file, image_dir, mask_dir, test_size=0.15, val_size=0.15, batch_size=2, random_state=420):\n",
    "    \"\"\"\n",
    "    Create train, validation and test splits while keeping all images from the same bat together.\n",
    "    Uses enhanced dataset with preprocessing.\n",
    "    \"\"\"\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Get unique bat IDs\n",
    "    unique_bats = df['BatID'].unique()\n",
    "    \n",
    "    # First split off the test set\n",
    "    train_val_bats, test_bats = train_test_split(\n",
    "        unique_bats, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Then split the remaining data into train and validation\n",
    "    train_bats, val_bats = train_test_split(\n",
    "        train_val_bats,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create full dataset with enhanced preprocessing\n",
    "    full_dataset = EnhancedVeinDataset(csv_file, image_dir, mask_dir)\n",
    "    \n",
    "    # Get indices for each split\n",
    "    train_indices = df[df['BatID'].isin(train_bats)].index.tolist()\n",
    "    val_indices = df[df['BatID'].isin(val_bats)].index.tolist()\n",
    "    test_indices = df[df['BatID'].isin(test_bats)].index.tolist()\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Print split information\n",
    "    print(\"\\nDataset Split Information:\")\n",
    "    print(f\"Total number of bats: {len(unique_bats)}\")\n",
    "    print(f\"Number of training bats: {len(train_bats)}\")\n",
    "    print(f\"Number of validation bats: {len(val_bats)}\")\n",
    "    print(f\"Number of test bats: {len(test_bats)}\")\n",
    "    print(f\"\\nTotal number of images: {len(df)}\")\n",
    "    print(f\"Number of training images: {len(train_indices)}\")\n",
    "    print(f\"Number of validation images: {len(val_indices)}\")\n",
    "    print(f\"Number of test images: {len(test_indices)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571aa64b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4deb8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, val_losses):\n",
    "    \"\"\"Plot training and validation loss history\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Training loop\n",
    "def train_model_enhanced(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                num_epochs, \n",
    "                device,\n",
    "                save_path='best_bifpn_model.pth'):\n",
    "    \"\"\"\n",
    "    Training loop for U-Net with Bi-FPN model\n",
    "    \"\"\"\n",
    "    # Initialize best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Initialize lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training at {datetime.datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Main epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        print(\"Training...\")\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if batch_count % 10 == 0:\n",
    "                print(f\"Batch {batch_count}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        print(\"\\nValidating...\")\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader):\n",
    "                # Move data to device\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(predictions, masks)\n",
    "                \n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                batch_count += 1\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs} Summary:')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, save_path)\n",
    "            print(f'Saved new best model with validation loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Final plot\n",
    "    plot_training_history(train_losses, val_losses)\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Function to evaluate model performance with metrics from paper\n",
    "def evaluate_model_comprehensive(model, test_loader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation with metrics used in the paper:\n",
    "    - Sensitivity (SE)\n",
    "    - Specificity (SP)\n",
    "    - Accuracy (ACC)\n",
    "    - AUC\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # For calculating pixel-wise metrics\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    total_tn = 0\n",
    "    \n",
    "    print(\"Evaluating model on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, masks)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Convert to binary predictions using threshold\n",
    "            binary_preds = (predictions > threshold).float()\n",
    "            \n",
    "            # Collect for ROC curve calculation\n",
    "            all_preds.extend(predictions.cpu().numpy().flatten())\n",
    "            all_masks.extend(masks.cpu().numpy().flatten())\n",
    "            \n",
    "            # Update confusion matrix values\n",
    "            for i in range(len(images)):\n",
    "                pred = binary_preds[i].view(-1)\n",
    "                mask = masks[i].view(-1)\n",
    "                \n",
    "                total_tp += (pred * mask).sum().item()\n",
    "                total_fp += (pred * (1 - mask)).sum().item()\n",
    "                total_fn += ((1 - pred) * mask).sum().item()\n",
    "                total_tn += ((1 - pred) * (1 - mask)).sum().item()\n",
    "    \n",
    "    # Calculate average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sensitivity = total_tp / (total_tp + total_fn + 1e-5)\n",
    "    specificity = total_tn / (total_tn + total_fp + 1e-5)\n",
    "    accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn + 1e-5)\n",
    "    \n",
    "    # Calculate AUC if sklearn is available\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc_score = roc_auc_score(np.array(all_masks) > 0.5, all_preds)\n",
    "    except:\n",
    "        auc_score = 0.0\n",
    "        print(\"sklearn not available, AUC not calculated\")\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Sensitivity (SE): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity (SP): {specificity:.4f}\")\n",
    "    print(f\"Accuracy (ACC): {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    results = {\n",
    "        'test_loss': avg_test_loss,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Model initialization and training example\n",
    "def setup_training(model, learning_rate=1e-4, loss_type='combined'):\n",
    "    \"\"\"\n",
    "    Set up loss function and optimizer for training\n",
    "    \n",
    "    Args:\n",
    "        model: The U-Net model\n",
    "        learning_rate: Learning rate for the optimizer\n",
    "        loss_type: One of 'bce', 'dice', or 'combined'\n",
    "        \n",
    "    Returns:\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "    \"\"\"\n",
    "    # Set up loss function\n",
    "    if loss_type == 'bce':\n",
    "        criterion = nn.BCELoss()\n",
    "    elif loss_type == 'dice':\n",
    "        criterion = DiceLoss()\n",
    "    elif loss_type == 'combined':\n",
    "        criterion = CombinedLoss(dice_weight=0.2, bce_weight=0.8)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "        verbose=True,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    return criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23b004",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a99d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, test_loader, device, num_examples=4):\n",
    "    \"\"\"\n",
    "    Make predictions on test data and visualize the results with overlaid masks\n",
    "    \n",
    "    Args:\n",
    "        model: Trained U-Net model\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Device to run inference on (cuda or cpu)\n",
    "        num_examples: Number of examples to visualize\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4 * num_examples))\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    images, true_masks = next(iter(test_loader))\n",
    "    \n",
    "    # If we have fewer examples than requested, adjust\n",
    "    num_examples = min(num_examples, len(images))\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for i in range(num_examples):\n",
    "            # Get a single example\n",
    "            image = images[i:i+1].to(device)\n",
    "            true_mask = true_masks[i:i+1].to(device)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred_mask = model(image)\n",
    "            \n",
    "            # Move everything back to CPU for visualization\n",
    "            image = image.cpu().squeeze(0)\n",
    "            true_mask = true_mask.cpu().squeeze(0)\n",
    "            pred_mask = pred_mask.cpu().squeeze(0)\n",
    "            \n",
    "            # Convert tensors to numpy arrays\n",
    "            image_np = image.squeeze().numpy()\n",
    "            true_mask_np = true_mask.squeeze().numpy()\n",
    "            pred_mask_np = pred_mask.squeeze().numpy()\n",
    "            \n",
    "            # Create overlay image (predicted mask in red)\n",
    "            overlay = np.zeros((*image_np.shape, 3))\n",
    "            overlay[..., 0] = image_np  # Red channel\n",
    "            overlay[..., 1] = image_np  # Green channel\n",
    "            overlay[..., 2] = image_np  # Blue channel\n",
    "            \n",
    "            # Add mask in red with 50% opacity (adjust color and alpha as needed)\n",
    "            overlay[..., 0] = np.maximum(image_np, pred_mask_np * 0.7)\n",
    "            overlay[..., 1] = image_np * (1 - pred_mask_np * 0.5)\n",
    "            overlay[..., 2] = image_np * (1 - pred_mask_np * 0.5)\n",
    "            \n",
    "            # Ensure overlay values are within range\n",
    "            overlay = np.clip(overlay, 0, 1)\n",
    "            \n",
    "            # Display images\n",
    "            axes[i, 0].imshow(image_np, cmap='gray')\n",
    "            axes[i, 0].set_title('Original Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(true_mask_np, cmap='gray')\n",
    "            axes[i, 1].set_title('True Mask')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred_mask_np, cmap='gray')\n",
    "            axes[i, 2].set_title('Predicted Mask')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(overlay)\n",
    "            axes[i, 3].set_title('Overlay (Pred Mask)')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4114c1",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset/dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     17\u001b[0m criterion, optimizer, scheduler \u001b[38;5;241m=\u001b[39m setup_training(\n\u001b[0;32m     18\u001b[0m     model,\n\u001b[0;32m     19\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     20\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Use combined loss as in the paper\u001b[39;00m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Create data loaders with enhanced preprocessing\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_val_test_splits_enhanced\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset/dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset/Images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset//Masks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model_enhanced(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     42\u001b[0m )\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mcreate_train_val_test_splits_enhanced\u001b[1;34m(csv_file, image_dir, mask_dir, test_size, val_size, batch_size, random_state)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mCreate train, validation and test splits while keeping all images from the same bat together.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mUses enhanced dataset with preprocessing.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get unique bat IDs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m unique_bats \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Robby\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset/dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Example usage with adaptive number of levels:\n",
    "\n",
    "# Initialize improved model with automatic level limitation\n",
    "# For 48x48 patches, we should only use 3-4 levels to avoid 1x1 feature maps\n",
    "model = UNetBiFPN(\n",
    "    in_channels=1,  # 1 for grayscale\n",
    "    out_channels=1, # 1 for binary segmentation\n",
    "    features=[32, 64, 128, 256, 512],  # Feature dimensions at each level\n",
    "    max_levels=4  # Limit to 4 levels max for 48x48 input\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup with combined loss\n",
    "criterion, optimizer, scheduler = setup_training(\n",
    "    model,\n",
    "    learning_rate=0.001,\n",
    "    loss_type='combined'  # Use combined loss as in the paper\n",
    ")\n",
    "\n",
    "# Create data loaders with enhanced preprocessing\n",
    "train_loader, val_loader, test_loader = create_train_val_test_splits_enhanced(\n",
    "    csv_file='../Dataset/dataset.csv',\n",
    "    image_dir='../Dataset/Images',\n",
    "    mask_dir='../Dataset//Masks',\n",
    "    test_size=0.2,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Train model\n",
    "train_losses, val_losses = train_model_enhanced(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=25,  # As in the paper, using fewer epochs than original U-Net\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "results = evaluate_model_comprehensive(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Final results: SP={results['specificity']:.4f}, SE={results['sensitivity']:.4f}, \"\n",
    "      f\"ACC={results['accuracy']:.4f}, AUC={results['auc']:.4f}\")\n",
    "\n",
    "predict_and_visualize(model, test_loader, device, num_examples=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
